#+SETUPFILE: ~/Dropbox/org-minimal-html-theme/org-minimal-theme.setup
#+HTML_HEAD: <style type="text/css">.example {background-color: #eff0f1; color: #ff0000;}</style>
#+TITLE: Refactoring  Python Google Maps Access Code

So, I've been working on analysis of property prices in Ireland recently [fn:1],
and I sourced some code from Shane Lynn, who did this before [fn:2]. 

Unfortunately, he stopped doing this in 2016 (back when Google Maps was still free). 

So, as part of my work, I ran this script up till October 2018 to update
to the (then) most recent date. 

However, that was a long time ago now, and I actually want to make this easier
for myself in the future by automating it.

Unfortunately, it's just a script that doesn't take arguments and doesn't really
lend itself well to extension. While I don't think I'll need to change
it too much in the future, I would like to be able to update the database
at the flick of a button so I can build tools on top of the database. 

Additionally, there's a commercial leases register that I'd like to
give the same treatment. In order to do this, I'm going to take
the advice of the esteemed Kent Beck: 
#+BEGIN_QUOTE
First make the change easy (warning: this may be hard!); then make the easy change
#+END_QUOTE

In general, this article talks about refactoring in the Martin Fowler sense:
#+BEGIN_QUOTE
code refactoring is the process of restructuring existing computer
code — changing the factoring — without changing its external behavior
#+END_QUOTE

So, this imposes some constraints on us. We *must not* modify the behaviour
while improving the structure. In order to make this work, we need some
tests to assess what the current behaviour is.

Unfortunately, we don't have any tests for this script, except for
the already existing tests (the old data). Additionally, each network
call we make costs money so we want to improve this script without
costing ourselves too much money [fn:3]

I'm going to approach this in a reasonably structured way. Firstly,
we'll review each piece of the script, and assess what would
make it more maintainable, or at least more maintainable. 

I'll reference particular refactorings from the book of the same
name [fn:4]. These have a number associated with them, which
is the page number from the 2nd edition. 

** Introduction & Imports

#+BEGIN_SRC python 
  """Python script for batch geocoding of addresses using the Google
  Geocoding API.  This script allows for massive lists of addresses to
  be geocoded for free by pausing when the geocoder hits the free rate
  limit set by Google (2500 per day).  If you have an API key for paid
  geocoding from Google, set it in the API key section.  Addresses for
  geocoding can be specified in a list of strings "addresses". In this
  script, addresses come from a csv file with a column "Address". Adjust
  the code to your own requirements as needed.  After every 500
  successul geocode operations, a temporary file with results is
  recorded in case of script failure / loss of connection later.
  Addresses and data are held in memory, so this script may need to be
  adjusted to process files line by line if you are processing millions
  of entries.  Shane Lynn 5th November 2016 """

  import pandas as pd
  import requests

  import time
#+END_SRC

This is pretty unremarkable, except that the comments are now lies. 

We should probably change this, but commenting the overall 
script is a task best done at the end of this process, to 
avoid it becoming misleading (as in this case). 

Current step: delete comments. 

#+BEGIN_SRC python 
 logger = logging.getLogger("root")
 logger.setLevel(logging.DEBUG)
 # create console handler
 ch = logging.StreamHandler()
 ch.setLevel(logging.DEBUG)
 logger.addHandler(ch)
#+END_SRC

The next portion of the code sets up a logger. 

This doesn't really need to be at the top level, so we
can abstract it into a function. Essentially, we're
doing the extract function refactoring here.

This is a nice easy one, as the logger isn't
mutated or used in multiple places further on. 

#+BEGIN_SRC python :tangle geocode/geocode_funcs.py
  import logging
  import requests
  import pandas as pd
  def create_logger():
      logger = logging.getLogger("root")
      logger.setLevel(logging.DEBUG)
      # create console handler
      ch = logging.StreamHandler()
      ch.setLevel(logging.DEBUG)
      logger.addHandler(ch)
      return logger


#+END_SRC

We could update this to handle different log levels,
but that's a bigger change so we'll avoid doing it for now. 

Given that we have a function, we can actually write 
a test. In general, it's better to write these tests
before the code (i.e. Test-Driven-Development), but
when working with legacy code (even small pieces of it like this),
this tends to be less practical. 

Before we can write and run tests easily, we need to create a directory
to hold our code and tests. We also need to invoke the ghost of python
actions, __init__.py. 

#+BEGIN_SRC sh
mkdir geocode
mkdir geocode/tests/
touch geocode/tests/__init__.py
#+END_SRC

#+RESULTS:


Now we can add a test file, and start moving code from the top level file
into the package, adding tests as we go. 

#+BEGIN_SRC python :tangle geocode/tests/test_google.py
import pytest
from geocode_funcs import create_logger, get_api_key
#+END_SRC

#+BEGIN_SRC python :tangle geocode/tests/test_google.py

  def test_logger_is_created() -> None:
      logger = create_logger()
      assert logger is not None


#+END_SRC

This is a crappy test, but we'll improve it as we go. The important
thing is to start adding tests, as they make it easier to iterate. 

#+BEGIN_SRC python
 key_file = open("key.txt", "r")
 key = key_file.readline().strip()
 API_KEY = key
#+END_SRC

The next part of the code deals with authentication.
We don't want to put keys in the scripts, as that's a big security
risk. 

We should create a function here, which takes a path and returns a string
with the key. First we'll write another test, so that we know
what we're doing. 

#+BEGIN_SRC python :tangle geocode/tests/test_google.py
  def test_get_api_key() -> None:
      path = 'key.txt'
      apikey = get_api_key(path)
      assert isinstance(apikey, str)
#+END_SRC



We run this test, and it fails because the function doesn't exist.
Let's add it now. 


#+BEGIN_SRC python :tangle geocode/geocode_funcs.py
  def get_api_key(path) -> str:
      key_file = open(path, "r")
      key = key_file.readline().strip()
      return key


#+END_SRC


Again, this is just mostly mechanical. 

The only real art to it is deciding what to break down, and what to
name stuff. In general, making changes like this is dangerous,
so you want to take a bunch of small steps and add tests, to 
avoid causing more problems than you solve. 



The next portion of the code handles options

#+BEGIN_SRC python
BACKOFF_TIME = 30
 # Set your output file name here.
 output_filename = 'output_full_2018_19.csv'
 # Set your input file here
 input_filename = 'prop2018-19_todo.csv'
 # Specify the column name in your input data that contains addresses here
 address_column_name = "address"
 # Return Full Google Results? If True, full JSON results from Google are included in output
 RETURN_FULL_RESULTS = True
#+END_SRC

In general, these should probably be arguments (using argparse).

Normally, I wouldn't make this change now, but as you may or may not
recall, we actually haven't run the whole process end-to-end
yet. [fn:5]. In order to make testing easy, we need to add these
options now (so that we can easily pass in test data). 

We'll use [[https://docs.python.org/3/howto/argparse.html][argparse]], which is included in the python standard library. 

Argpase essentially uses a Parser object, to which one
adds arguments which can then be used on the command line. 

It's essentially a nicer version of $1, $2 etc. 

One thing that strikes me now is that I'm not really sure how to test
this kind of code (i.e. argument parsing). Everything that I can
think of seems a little redundant, to be honest. 

For now, we'll rush ahead, but keep the lack of tests around this
part of the code in mind for the future. 


#+BEGIN_SRC python 
import argparse
parser = argparse.ArgumentParser()
args = parser.parse_args()
#+END_SRC


Hmmm, interesting. My approach to this script was to break down the
functionality into a library of functions, and then replace the
inline code with calls to the (tested) functions. 

Clearly this won't work with the argument parser, as it's by definition
embedded within the top-level script. So we're actually gonna
need to back up here, and get some (small) test data with known answers
(i.e. already parsed) so that we can ensure we don't break anything
when we change this. [fn:6]. 

** Generating Test Data

The best test data is actual data that you already log. 
In many cases, this may be the only thing that you can reliably get your hands on.

In this particular situation, things aren't so bad. The results we
get are determined only by the input file and one file of code 
which isn't too tangled. In some other systems, even getting your
hands on test data represents an awful lot of work. 

So, let's examine our input and output data.

The original script has the following bunch of options. 

#+BEGIN_SRC python :exports code
 # Set your output file name here.
 output_filename = 'output_full_2018_19.csv'
 # Set your input file here
 input_filename = 'prop2018-19_todo.csv'
#+END_SRC

So, we take in data from the input filename, and put it out to
output_filename [fn:7].

When we look at the input file name, we see the following. 

#+BEGIN_SRC sh :exports results
wc -l prop2018-19_todo.csv
#+END_SRC

#+RESULTS:
: 37188 prop2018-19_todo.csv

OK, so we have 37k input rows. This would be the perfect test case,
but it's impractical as it would take a /loooooonnng/ time to generate,
and would cost me more money than I'm prepared to spend for a test run. 

We'll subset the data to just take five lines right now, to give us 
a small functional test we can run to make sure that we're not
breaking the application in the course of refactoring. 

#+BEGIN_SRC sh 
head -5 prop2018-19_todo.csv > input_sample_data.csv
#+END_SRC

#+RESULTS:


#+BEGIN_SRC sh :exports results
head input_sample_data.csv
#+END_SRC

#+RESULTS:
| date_of_sale_dd_mm_yyyy | address                               | postal_code | county    |    price | not_full_market_price | vat_exclusive | description_of_property               | property_size_description |
| 09/11/2018              | KNOCKEIL, RATHDOWNEY, CO. LAOIS       | NA          | Laois     |  4500000 | No                    | No            | Second-Hand Dwelling house /Apartment | NA                        |
| 09/11/2018              | KNOCKGRAFFON, CHAIR, CO TIPPERARY     | NA          | Tipperary | 20000000 | No                    | No            | Second-Hand Dwelling house /Apartment | NA                        |
| 09/11/2018              | KNOCKNABINNY, BARRELLS CROSS, KINSALE | NA          | Cork      | 64000000 | No                    | No            | Second-Hand Dwelling house /Apartment | NA                        |
| 09/11/2018              | KNOCKNORAN, KILMORE, WEXFORD          | NA          | Wexford   | 20000000 | No                    | No            | Second-Hand Dwelling house /Apartment | NA                        |


So we can see that this these are rows from the PPR. We don't really need most of this data,
but it's easier to just keep it (according to our principle of minimising changes). 

Next, let's look at the output data. 

#+BEGIN_SRC sh :exports results
wc -l output_full_2018_19.csv
#+END_SRC

#+RESULTS:
: 10001 output_full_2018_19.csv

Oddly, we have less output rows. I /think/ that's because I ran this
three times as it kept breaking, but I don't appear to have any 
notes to remind myself of this fact [fn:8]. 

Hopefully, the first output rows match our first input rows. 

#+BEGIN_SRC sh
awk -F, '{print $2, $5}' output_full_2018_19.csv
#+END_SRC

And here, children is where I remember that I hate awk when dealing
with formatted CSVs. The problem always ends up being commas in input
data. This is particularly common in addresses, which is what
we're dealing with here. Ah well, fortunately there are other tools. 

My normal response to this kind of thing would be R, but given that
we're using python here I guess I'll have to content myself with
pandas. 

#+BEGIN_SRC python :session
  import pandas as pd
  output_data = pd.read_csv("output_full_2018_19.csv")
  input_data = pd.read_csv("input_sample_data.csv")
  output_data.keys()
#+END_SRC

#+RESULTS:
: Index(['Unnamed: 0', 'accuracy', 'formatted_address', 'google_place_id',
:        'input_string', 'latitude', 'longitude', 'number_of_results',
:        'postcode', 'response', 'status', 'type'],
:       dtype='object')

So, we want formatted address and input string, for now. 

#+BEGIN_SRC python :session
  address_input_output = output_data[["input_string", "formatted_address"]]
  address_input_output.head()
#+END_SRC

#+RESULTS:
:                                         input_string                              formatted_address
: 0      KNOCKEIL, RATHDOWNEY, CO. LAOIS,Laois,Ireland    Knockiel Dr, Rathdowney, Co. Laois, Ireland
: 1  KNOCKGRAFFON, CHAIR, CO TIPPERARY,Tipperary,Ir...  Knockgraffon, New Inn, Co. Tipperary, Ireland
: 2  KNOCKNABINNY, BARRELLS CROSS, KINSALE,Cork,Ire...                Knocknabinny, Co. Cork, Ireland
: 3       KNOCKNORAN, KILMORE, WEXFORD,Wexford,Ireland               Knocknoran, Co. Wexford, Ireland
: 4               LACKA, SHINRONE, BIRR,Offaly,Ireland                  Lacka, Co. Tipperary, Ireland

** Functional Testing

OK, this looks good, in that we have the same addresses as in the input data.
This means that we actually have a base set of data to compare our results
to, which is critically important if we're going to avoid breakages. 

However, we don't want to be comparing strings to one another in our
tests, as that's tedious and error-prone. The question of what to test
is one that deserves more attention than it gets. Many of us seem to
either be test nothing or test everything, whereas most of the interesting
questions are around how can we get the maximum impact from minimum
tests. 



Normally what I'd do here is use some kind of dataframe equality
method on the outputs. We save our original output under a particular
name (I'm a fan of output_old, myself) and then we rerun the process
that generates the script. 

Pandas has a df.equals method, so hopefully this will _just work_. 

We need to run the original script on the sample addresses, to
create an up to date record of what the results were.

Following this, each time we make a significant change we can run
this functional test and make sure that nothing has broken. 

#+BEGIN_SRC sh
python geocode_original.py 
#+END_SRC

This runs the Gmaps API on the four input addresses and returns the data.
Note that I needed to alter the original script to have the same data. 



#+BEGIN_SRC python :session
  output_old = pd.read_csv('output_sample_data.csv')
  output_check = output_data.iloc[0:4,]
  output_check = output_data.iloc[0:4,]
  output_check.equals(output_old)

#+END_SRC

#+RESULTS:
: False

So, it doesn't work; but it doesn't tell us why it doesn't work
(unlike *all.equal* in R).That's irritating, but data science and
technology in general tends to be a world of irritations so we'll
move on and check. 

One good comparison would be the lat-long variables returned from Geocoding.
We wouldn't expect these to change over time (unless Google start identifying
addresses better at some point), and they are numeric and thus less error-prone
to compare (after accounting for the vagaries of float and rounding methods). 

#+BEGIN_SRC python :session
  latlong_check = output_check[["latitude", "longitude"]]
  latlong_old = output_old[["latitude", "longitude"]]
  latlong_check.equals(latlong_old)
#+END_SRC

#+RESULTS:
: False

Still false. Odd, let's look at the data to see what's going on here. 

#+BEGIN_SRC python :session
latlong_check.head()
#+END_SRC

#+RESULTS:
:     latitude  longitude
: 0  52.851635  -7.589150
: 1  52.422151  -7.924781
: 2  51.687483  -8.572435
: 3  52.210920  -6.564558

#+BEGIN_SRC python :session
latlong_old.head()
#+END_SRC

#+RESULTS:
:     latitude  longitude
: 0  52.851635  -7.589150
: 1  52.422151  -7.924781
: 2  51.687563  -8.564443
: 3  52.210920  -6.564558


So, this is a complete waste of time, lets take a different approach. 

** Functional Testing in R

I can do this in approximately 5 lines of R, so that's what I'm going to do.
I probably now need some kind of orchestration script, but we'll run things
manually for now. 

#+BEGIN_SRC R :session :results none :tangle functional_check.R
  require(testthat)
  options(warn=-1)
  args <- commandArgs(trailingOnly=TRUE)
  output_old  <-  suppressMessages(readr::read_csv(args[[1]]))
  output_new <- suppressMessages(readr::read_csv(args[[2]]))
  print(test_that("sample files are equal", 
                  expect_equal(output_old, output_new)))
#+END_SRC

Five lines of R later. To be fair, equality is tricky business, so I'm
not surprised its not quite as easy in Python (as it's not as
domain-specific as R). The great thing about R is the all.equal
methods, which make implementing our test a breeze [fn:9]. 

This script can be run as follows:

#+BEGIN_SRC sh :exports both
Rscript functional_check.R "output_sample_data.csv" "output_sample_data2.csv"
#+END_SRC


And will return either TRUE or FALSE. 
** Back to Options

You may remember that before this digression, we were considering
options. The argparse module was our chosen approach, but
we noted that we'd need to put these at the top level of our script
in order to make them work. Therefore, we need to create new file,
called *geocoder_new.py*. 

Next, we need to add some options. 

#+BEGIN_SRC python
BACKOFF_TIME = 30
 # Set your output file name here.
 output_filename = 'output_full_2018_19.csv'
 # Set your input file here
 input_filename = 'prop2018-19_todo.csv'
 # Specify the column name in your input data that contains addresses here
 address_column_name = "address"
 # Return Full Google Results? If True, full JSON results from Google are included in output
 RETURN_FULL_RESULTS = True
#+END_SRC

So we have five options to add. The first, backoff time is kinda irrelevant now
that Google charge you for each request (not each request over a quota). We'll
leave it as part of the script for now, but it's definitely worth removing
once we have more tests in place. We need output and input filenames, definitely,
and the address_column_name is also important. These are all strings, which
is the default datatype returned by argparse, so that's nice. Finally we
have return_full_results, which is a boolean. 

#+BEGIN_SRC python :results none
import argparse
parser = argparse.ArgumentParser()
parser.add_argument("--backoff_time", type=int, help="backoff time", default=30)
parser.add_argument("--input_file", type=str, help="input file containing addresses", default=None)
parser.add_argument("--output_file", type=str, help="file to output geocoded results too", default=None)
parser.add_argument("--address_column", type=str, help="column name with addresses", default=None)
parser.add_argument("--return_full_results", type=bool, help="return full results?", default=True)
args = parser.parse_args()
#+END_SRC

#+begin_src sh 
  python geocoder_new.py
  --input_file=input_sample_data_one.csv //
  --output_file=output_sample_data_one.csv //
  --address_column=address 
#+end_src

We can run our new script as above. 

If you remember, we had one function in the original
script. Let's take a look at it. 

#+begin_src python :tangle geocode/geocode_funcs.py
def get_google_results(address, api_key=None, return_full_response=False):
    """Get geocode results from Google Maps Geocoding API.
    Note, that in the case of multiple google geocode reuslts, 
    this function returns details of the FIRST result.
    @param address: String address as accurate as possible. For
    Example "18 Grafton Street, Dublin, Ireland" 
    @param api_key:
    String API key for Google Maps Platform
    @param
    return_full_response: Boolean to indicate if you'd like to return
    the full response from google. This is useful if you'd like
    additional location details for storage or parsing later.

    """
    # Set up your Geocoding url
    geocode_url = "https://maps.googleapis.com/maps/api/geocode/json?address={}".format(address)
    if api_key is not None:
        geocode_url = geocode_url + "&key={}".format(api_key)

    # Ping google for the reuslts:
    results = requests.get(geocode_url)
    # Results will be in JSON format - convert to dict using requests functionality
    results = results.json()

    # if there's no results or an error, return empty results.
    if len(results['results']) == 0:
        output = {
            "formatted_address" : None,
            "latitude": None,
            "longitude": None,
            "accuracy": None,
            "google_place_id": None,
            "type": None,
            "postcode": None
        }
    else:    
        answer = results['results'][0]
        output = {
            "formatted_address" : answer.get('formatted_address'),
            "latitude": answer.get('geometry').get('location').get('lat'),
            "longitude": answer.get('geometry').get('location').get('lng'),
            "accuracy": answer.get('geometry').get('location_type'),
            "google_place_id": answer.get("place_id"),
            "type": ",".join(answer.get('types')),
            "postcode": ",".join([x['long_name'] for x in answer.get('address_components') 
                                  if 'postal_code' in x.get('types')])
        }

    # Append some other details:    
    output['input_string'] = address
    output['number_of_results'] = len(results['results'])
    output['status'] = results.get('status')
    if return_full_response is True:
        output['response'] = results

    return output
#+end_src

We move this to our library, and update the script. There's 
a /lot/ of logic here that processes the results, which makes the name
somewhat deceiving, but let's leave that for now [fn:10]. 



We next update our new script to import this function into the main script. 


We run again, ensure everything is working, and continue.
** More Unit Tests

At this point, we should add some tests. The *get_google_results* is the core
of this program, and as such we should make sure that we get good coverage
here. 

There's a couple of things that stand out as needing testing:
- The type of response
- The length of the response: note the doc string
- what happens if there is no address
- full response vs non-full response:
  - differences in returned fields
  - differences in names and shape of said fields

We can skip a bunch of this by just assuming that the results are
correct; and testing that. This is normally called characterisation
testing. Note that this is less useful in small examples like this,
but can be incredibly effective in larger systems. 

The basic idea is that you get some data from the system, like
a geocoded address, save it and then run the test by comparing
the original output to the new output. This is hacky, not pretty
and ruthlessly effective (in the short term). Long term, you
want to actually test the designed functionality (i.e. does the system
do what it's supposed to), but when refactoring, an assurance of
no changes is probably just as useful. 

In order to do this, we need to write a function that 
saves our results in some format. We'll use *pickle* for this
purpose, as it's standard and reasonably simple. 

The way python handles files bugs me normally, so we'll
write a wrapper around the with statement dance we need. 

#+begin_src python :tangle geocode/geocode_funcs.py
  import pickle
  def write_results_to_pickle(results, filename):
      with open(filename, 'wb') as f:
          pickle.dump(results, f)

  def read_results_from_pickle(filename):
      with open(filename, 'rb') as f:
          res = pickle.load(f)
      return res
#+end_src

I really only created these functions


#+begin_src python :tangle geocode/tests/test_google.py
  from geocode_funcs import get_google_results, get_api_key, read_results_from_pickle
  key = get_api_key('/home/richie/Dropbox/Code/Python/geocoder/key.txt')
  def test_nonfull_results():
      kilcanway_nonfull = get_google_results("Kilcanway, Mallow, Co. Cork, Ireland",
                                             api_key = key,
                                             return_full_response = False)
      oldnon_full = read_results_from_pickle('nonfull_kilcanway.pkl')
      assert kilcanway_nonfull == oldnon_full

  def test_full_results():
      kilcanway_full = get_google_results("Kilcanway, Mallow, Co. Cork, Ireland",
                                             api_key = key,
                                             return_full_response = True)
      old_full = read_results_from_pickle('full_kilcanway.pkl')
      assert kilcanway_full == old_full    

#+end_src

Some copying and creation of files later, we have some tests. 

We probably also need a test to make sure that we break if there's 
no API key, but again, we'll leave that for now.
** Reviewing where we are

Let's look at our new geocoding script in all its glory.

The first thing that stands out is the presence of a test inlined
in the code. We can easily move this to be part of our automated
testing suite, with a few small changes. We note this down,
and continue reviewing the script. 

#+begin_src python
  import pandas as pd
  import logging
  import time
  from geocode.geocode_funcs import create_logger, get_api_key, get_google_results
  import argparse
  parser = argparse.ArgumentParser()
  parser.add_argument("--backoff_time", type=int, help="backoff time", default=30)
  parser.add_argument("--input_file", type=str, help="input file containing addresses", default=None)
  parser.add_argument("--output_file", type=str, help="file to output geocoded results too", default=None)
  parser.add_argument("--address_column", type=str, help="column name with addresses", default=None)
  parser.add_argument("--return_full_results", type=bool, help="return full results?", default=True)
  args = parser.parse_args()
  logger = create_logger()

  key = get_api_key("key.txt")
  API_KEY = key

  BACKOFF_TIME = args.backoff_time
  # Set your output file name here.
  output_filename = args.output_file
  # Set your input file here
  input_filename = args.input_file
  # Specify the column name in your input data that contains addresses here
  address_column_name = args.address_column
  # Return Full Google Results? If True, full JSON results from Google are included in output
  RETURN_FULL_RESULTS = args.return_full_results

  #------------------ DATA LOADING --------------------------------

  # Read the data to a Pandas Dataframe
  data = pd.read_csv(input_filename, encoding='utf8')

  if address_column_name not in data.columns:
          raise ValueError("Missing Address column in input data")

  # Form a list of addresses for geocoding:
  # Make a big list of all of the addresses to be processed.
  addresses = data[address_column_name].tolist()

  # **** DEMO DATA / IRELAND SPECIFIC! ****
  # We know that these addresses are in Ireland, and there's a column for county, so add this for accuracy. 
  # (remove this line / alter for your own dataset)
  addresses = (data[address_column_name] + ',' + data['county'] + ',Ireland').tolist()



  #------------------ PROCESSING LOOP -----------------------------

  # Ensure, before we start, that the API key is ok/valid, and internet access is ok
  test_result = get_google_results("London, England", API_KEY, RETURN_FULL_RESULTS)
  if (test_result['status'] != 'OK') or (test_result['formatted_address'] != 'London, UK'):
      logger.warning("There was an error when testing the Google Geocoder.")
      raise ConnectionError('Problem with test results from Google Geocode - check your API key and internet connection.')

  # Create a list to hold results
  results = []
  # Go through each address in turn
  for address in addresses:
      # While the address geocoding is not finished:
      geocoded = False
      while geocoded is not True:
          # Geocode the address with google
          try:
              geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)
          except Exception as e:
              logger.exception(e)
              logger.error("Major error with {}".format(address))
              logger.error("Skipping!")
              geocoded = True

          # If we're over the API limit, backoff for a while and try again later.
          if geocode_result['status'] == 'OVER_QUERY_LIMIT':
              logger.info("Hit Query Limit! Backing off for a bit.")
              time.sleep(BACKOFF_TIME * 60) # sleep for 30 minutes
              geocoded = False
          else:
              # If we're ok with API use, save the results
              # Note that the results might be empty / non-ok - log this
              if geocode_result['status'] != 'OK':
                  logger.warning("Error geocoding {}: {}".format(address, geocode_result['status']))
              logger.debug("Geocoded: {}: {}".format(address, geocode_result['status']))
              results.append(geocode_result)           
              geocoded = True

      # Print status every 100 addresses
      if len(results) % 100 == 0:
          logger.info("Completed {} of {} address".format(len(results), len(addresses)))

      # Every 500 addresses, save progress to file(in case of a failure so you have something!)
      if len(results) % 500 == 0:
          pd.DataFrame(results).to_csv("{}_bak".format(output_filename))
          print("saved {r} results to file".format(r=len(results)))
      if len(results) % 10000 == 0:
              pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
  #All done
  logger.info("Finished geocoding all addresses")
  # Write the full results to csv using the pandas library.
  pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
#+end_src

So, most of the core loop is fine, in that I can't see an easy
way to break it into functions. However, the one part that
looks reasonably self contained is the logging of results
to stdout and/or a dataframe. I don't particularly like
the overwriting, but I can live with it. 

#+begin_src python :tangle geocode/geocode_funcs.py
def log_progress_and_results(results, logger, addresses, output_filename) -> None:
    if len(results) % 100 == 0:
        logger.info("Completed {} of {} address".format(len(results), len(addresses)))
    if len(results) % 500 == 0:
        pd.DataFrame(results).to_csv("{}_bak".format(output_filename))
        print("saved {r} results to file".format(r=len(results)))
    if len(results) % 10000 == 0:
        pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
#+end_src

This function is a little tricky to test, in that it has different behaviour
depending on the length of results. To test this, we'll need DF's of particular
lengths. We'll also need to remove an accumulation of files which will
build up as a result of the tests. 

The easiest thing to do here is to read in our full output data, and
subset it to be the correct length, and write tests around that. 

I /think/ that I could use pytest fixtures to do this, but in the
interests of finishing this article today, I'll leave that for now. 

#+begin_src python :tangle geocode/tests/test_google.py
  import pandas as pd
  from pathlib import Path
  import os
  from geocode_funcs import create_logger, log_progress_and_results
  resource_path = "/home/richie/Dropbox/Code/Python/geocoder" 
  output_data = pd.read_csv(os.path.join(resource_path, 'output_full_2018_19.csv'))
  output_data_100 = output_data.iloc[0:100,]
  output_data_500 = output_data.iloc[0:500,]
  output_data_10k = output_data
  output_filename = "test_output_file.csv"
  logger = create_logger()


  def test_output_data_exists():
      assert output_data is not None


  def test_output_length_100(caplog) -> None:
      log_progress_and_results(output_data_100, logger, output_data_100.input_string,
                               output_filename)
      assert 'Completed 100 of 100 address' in caplog.text


  def test_output_length_500() -> None:
      log_progress_and_results(output_data_500,
                               logger,
                               output_data_500.input_string,
                               output_filename)
      assert os.path.exists(output_filename + '_bak')

  def test_output_length_10000() -> None:
      log_progress_and_results(output_data_10k,
                               logger,
                               output_data_10k.input_string,
                               output_filename)
      assert os.path.exists(output_filename)
#+end_src

We run the tests, make sure they are still working. 

They are, so we can update the script and run again
to make sure nothing unexpected has broken. 

At this point, we're pretty much done (for now). 



Additionally, at this point I can now really easily update my script,
and even automate updating the database. 
* Conclusions

However, I probably want to start changing things at this point.

A few obvious extensions which would be relatively easy:
- apply approach to different country/dataset
- Add caching to script to prevent querying google if we already have
  a result (for resales)
- Write to a database instead of a flat file. 

Consider the possibility of writing to a DB. We can now just bundle
all the logic up into log_progress_and_results and the rest of the
code doesn't need to care. We'll need to update the tests, and
it would probably be worth adding an option so that we can
still output to a flat file if necessary, but this is much
easier than before.

The different country or dataset is a little tricker, because we
haven't abstracted the logic for addresses into a function. 

Additionally, it would be good to add a function for the main
loop, but that's lower value for now, as I can easily run
my new geocoder script on new data, which was the purpose 
of my refactoring in the first place. 

I j

* Full scripts :noexport:
** Original Script

 #+BEGIN_SRC python :tangle geocode_original.py
 """
 Python script for batch geocoding of addresses using the Google Geocoding API.
 This script allows for massive lists of addresses to be geocoded for free by pausing when the 
 geocoder hits the free rate limit set by Google (2500 per day).  If you have an API key for paid
 geocoding from Google, set it in the API key section.
 Addresses for geocoding can be specified in a list of strings "addresses". In this script, addresses
 come from a csv file with a column "Address". Adjust the code to your own requirements as needed.
 After every 500 successul geocode operations, a temporary file with results is recorded in case of 
 script failure / loss of connection later.
 Addresses and data are held in memory, so this script may need to be adjusted to process files line
 by line if you are processing millions of entries.
 Shane Lynn
 5th November 2016
 """

 import pandas as pd
 import requests
 import logging
 import time

 logger = logging.getLogger("root")
 logger.setLevel(logging.DEBUG)
 # create console handler
 ch = logging.StreamHandler()
 ch.setLevel(logging.DEBUG)
 logger.addHandler(ch)

 #------------------ CONFIGURATION -------------------------------

 # Set your Google API key here. 
 # Even if using the free 2500 queries a day, its worth getting an API key since the rate limit is 50 / second.
 # With API_KEY = None, you will run into a 2 second delay every 10 requests or so.
 # With a "Google Maps Geocoding API" key from https://console.developers.google.com/apis/, 
 # the daily limit will be 2500, but at a much faster rate.
 # Example: API_KEY = 'AIzaSyC9azed9tLdjpZNjg2_kVePWvMIBq154eA'
 key_file = open("key.txt", "r")
 key = key_file.readline().strip()
 API_KEY = key
 # Backoff time sets how many minutes to wait between google pings when your API limit is hit
 BACKOFF_TIME = 30
 # Set your output file name here.
 output_filename = 'output_sample_data2.csv'
 # Set your input file here
 input_filename = 'input_sample_data.csv'
 # Specify the column name in your input data that contains addresses here
 address_column_name = "address"
 # Return Full Google Results? If True, full JSON results from Google are included in output
 RETURN_FULL_RESULTS = True

 #------------------ DATA LOADING --------------------------------

 # Read the data to a Pandas Dataframe
 data = pd.read_csv(input_filename, encoding='utf8')

 if address_column_name not in data.columns:
	 raise ValueError("Missing Address column in input data")

 # Form a list of addresses for geocoding:
 # Make a big list of all of the addresses to be processed.
 addresses = data[address_column_name].tolist()

 # **** DEMO DATA / IRELAND SPECIFIC! ****
 # We know that these addresses are in Ireland, and there's a column for county, so add this for accuracy. 
 # (remove this line / alter for your own dataset)
 addresses = (data[address_column_name] + ',' + data['county'] + ',Ireland').tolist()


 #------------------	FUNCTION DEFINITIONS ------------------------

 def get_google_results(address, api_key=None, return_full_response=False):
     """
     Get geocode results from Google Maps Geocoding API.
    
     Note, that in the case of multiple google geocode reuslts, this function returns details of the FIRST result.
    
     @param address: String address as accurate as possible. For Example "18 Grafton Street, Dublin, Ireland"
     @param api_key: String API key if present from google. 
                     If supplied, requests will use your allowance from the Google API. If not, you
                     will be limited to the free usage of 2500 requests per day.
     @param return_full_response: Boolean to indicate if you'd like to return the full response from google. This
                     is useful if you'd like additional location details for storage or parsing later.
     """
     # Set up your Geocoding url
     geocode_url = "https://maps.googleapis.com/maps/api/geocode/json?address={}".format(address)
     if api_key is not None:
         geocode_url = geocode_url + "&key={}".format(api_key)
        
     # Ping google for the reuslts:
     results = requests.get(geocode_url)
     # Results will be in JSON format - convert to dict using requests functionality
     results = results.json()
    
     # if there's no results or an error, return empty results.
     if len(results['results']) == 0:
         output = {
             "formatted_address" : None,
             "latitude": None,
             "longitude": None,
             "accuracy": None,
             "google_place_id": None,
             "type": None,
             "postcode": None
         }
     else:    
         answer = results['results'][0]
         output = {
             "formatted_address" : answer.get('formatted_address'),
             "latitude": answer.get('geometry').get('location').get('lat'),
             "longitude": answer.get('geometry').get('location').get('lng'),
             "accuracy": answer.get('geometry').get('location_type'),
             "google_place_id": answer.get("place_id"),
             "type": ",".join(answer.get('types')),
             "postcode": ",".join([x['long_name'] for x in answer.get('address_components') 
                                   if 'postal_code' in x.get('types')])
         }
        
     # Append some other details:    
     output['input_string'] = address
     output['number_of_results'] = len(results['results'])
     output['status'] = results.get('status')
     if return_full_response is True:
         output['response'] = results
    
     return output

 #------------------ PROCESSING LOOP -----------------------------

 # Ensure, before we start, that the API key is ok/valid, and internet access is ok
 test_result = get_google_results("London, England", API_KEY, RETURN_FULL_RESULTS)
 if (test_result['status'] != 'OK') or (test_result['formatted_address'] != 'London, UK'):
     logger.warning("There was an error when testing the Google Geocoder.")
     raise ConnectionError('Problem with test results from Google Geocode - check your API key and internet connection.')

 # Create a list to hold results
 results = []
 # Go through each address in turn
 for address in addresses:
     # While the address geocoding is not finished:
     geocoded = False
     while geocoded is not True:
         # Geocode the address with google
         try:
             geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)
         except Exception as e:
             logger.exception(e)
             logger.error("Major error with {}".format(address))
             logger.error("Skipping!")
             geocoded = True
            
         # If we're over the API limit, backoff for a while and try again later.
         if geocode_result['status'] == 'OVER_QUERY_LIMIT':
             logger.info("Hit Query Limit! Backing off for a bit.")
             time.sleep(BACKOFF_TIME * 60) # sleep for 30 minutes
             geocoded = False
         else:
             # If we're ok with API use, save the results
             # Note that the results might be empty / non-ok - log this
             if geocode_result['status'] != 'OK':
                 logger.warning("Error geocoding {}: {}".format(address, geocode_result['status']))
             logger.debug("Geocoded: {}: {}".format(address, geocode_result['status']))
             results.append(geocode_result)           
             geocoded = True

     # Print status every 100 addresses
     if len(results) % 100 == 0:
    	 logger.info("Completed {} of {} address".format(len(results), len(addresses)))
            
     # Every 500 addresses, save progress to file(in case of a failure so you have something!)
     if len(results) % 500 == 0:
         pd.DataFrame(results).to_csv("{}_bak".format(output_filename))
         print("saved {r} results to file".format(r=len(results)))
     if len(results) % 10000 == 0:
             pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
 #All done
 logger.info("Finished geocoding all addresses")
 # Write the full results to csv using the pandas library.
 pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
 #+END_SRC
** New Script
#+BEGIN_SRC python :tangle geocoder_new.py
  import pandas as pd
  import logging
  import time
  from geocode.geocode_funcs import (create_logger, get_api_key, get_google_results,
                                     log_progress_and_results)
  import argparse
  parser = argparse.ArgumentParser()
  parser.add_argument("--backoff_time", type=int, help="backoff time", default=30)
  parser.add_argument("--input_file", type=str, help="input file containing addresses", default=None)
  parser.add_argument("--output_file", type=str, help="file to output geocoded results too", default=None)
  parser.add_argument("--address_column", type=str, help="column name with addresses", default=None)
  parser.add_argument("--return_full_results", type=bool, help="return full results?", default=True)
  args = parser.parse_args()
  logger = create_logger()

  #------------------ CONFIGURATION -------------------------------

  # Set your Google API key here. 
  # Even if using the free 2500 queries a day, its worth getting an API key since the rate limit is 50 / second.
  # With API_KEY = None, you will run into a 2 second delay every 10 requests or so.
  # With a "Google Maps Geocoding API" key from https://console.developers.google.com/apis/, 
  # the daily limit will be 2500, but at a much faster rate.
  # Example: API_KEY = 'AIzaSyC9azed9tLdjpZNjg2_kVePWvMIBq154eA'
  # key_file = open("key.txt", "r")
  # key = key_file.readline().strip()
  key = get_api_key("key.txt")
  API_KEY = key

  BACKOFF_TIME = args.backoff_time
  # Set your output file name here.
  output_filename = args.output_file
  # Set your input file here
  input_filename = args.input_file
  # Specify the column name in your input data that contains addresses here
  address_column_name = args.address_column
  # Return Full Google Results? If True, full JSON results from Google are included in output
  RETURN_FULL_RESULTS = args.return_full_results

  #------------------ DATA LOADING --------------------------------

  # Read the data to a Pandas Dataframe
  data = pd.read_csv(input_filename, encoding='utf8')

  if address_column_name not in data.columns:
          raise ValueError("Missing Address column in input data")

  # Form a list of addresses for geocoding:
  # Make a big list of all of the addresses to be processed.
  addresses = data[address_column_name].tolist()

  # **** DEMO DATA / IRELAND SPECIFIC! ****
  # We know that these addresses are in Ireland, and there's a column for county, so add this for accuracy. 
  # (remove this line / alter for your own dataset)
  addresses = (data[address_column_name] + ',' + data['county'] + ',Ireland').tolist()


  #------------------ PROCESSING LOOP -----------------------------

  # Ensure, before we start, that the API key is ok/valid, and internet access is ok
  test_result = get_google_results("London, England", API_KEY, RETURN_FULL_RESULTS)
  if (test_result['status'] != 'OK') or (test_result['formatted_address'] != 'London, UK'):
      logger.warning("There was an error when testing the Google Geocoder.")
      raise ConnectionError('Problem with test results from Google Geocode - check your API key and internet connection.')

  # Create a list to hold results
  results = []
  # Go through each address in turn
  for address in addresses:
      # While the address geocoding is not finished:
      geocoded = False
      while geocoded is not True:
          # Geocode the address with google
          try:
              geocode_result = get_google_results(address, API_KEY, return_full_response=RETURN_FULL_RESULTS)
          except Exception as e:
              logger.exception(e)
              logger.error("Major error with {}".format(address))
              logger.error("Skipping!")
              geocoded = True

          # If we're over the API limit, backoff for a while and try again later.
          if geocode_result['status'] == 'OVER_QUERY_LIMIT':
              logger.info("Hit Query Limit! Backing off for a bit.")
              time.sleep(BACKOFF_TIME * 60) # sleep for 30 minutes
              geocoded = False
          else:
              # If we're ok with API use, save the results
              # Note that the results might be empty / non-ok - log this
              if geocode_result['status'] != 'OK':
                  logger.warning("Error geocoding {}: {}".format(address, geocode_result['status']))
              logger.debug("Geocoded: {}: {}".format(address, geocode_result['status']))
              results.append(geocode_result)           
              geocoded = True

      #Print status every 100 addresses
      log_progress_and_results(results, logger, addresses, output_filename)
      # if len(results) % 100 == 0:
      #     logger.info("Completed {} of {} address".format(len(results), len(addresses)))

      # # Every 500 addresses, save progress to file(in case of a failure so you have something!)
      # if len(results) % 500 == 0:
      #     pd.DataFrame(results).to_csv("{}_bak".format(output_filename))
      #     print("saved {r} results to file".format(r=len(results)))
      # if len(results) % 10000 == 0:
      #         pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
  #All done
  logger.info("Finished geocoding all addresses")
  # Write the full results to csv using the pandas library.
  pd.DataFrame(results).to_csv(output_filename, encoding='utf8')
#+END_SRC

#+RESULTS:

* Footnotes



[fn:10] also, why do people name things output? It just seems like such a bizarre choice of name 

[fn:9] python's use of mutable references causes problems here

[fn:8] my original impetus to start testing was driven by the
difficulty of starting back up old side-projects

[fn:7]  and also some backup files, but that's for the future

[fn:6] stuff always breaks, it's better to make your peace with it. 

[fn:5] if you've been paying careful attention, you'll note that our
original script hasn't changed yet

[fn:4] if you have never read this book and you need to deal with
non-trivial code, you owe it to yourself to go read it *right now*

[fn:3] obviously, this threshold will be different for everyone

[fn:2] over time, the probability of an irish data scientist examining
this data converges towards one. 

[fn:1] i.e. since I decided to buy a house
